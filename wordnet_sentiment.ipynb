{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeba6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/s1155124614/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/s1155124614/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/s1155124614/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/s1155124614/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle \n",
    "import pprint \n",
    "\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d4a0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('String', 'ITEM 7', 'Found In Line', 2641)\n",
      "('String', 'ITEM 7', 'Found In Line', 3395)\n",
      "('String', 'ITEM 7A', 'Found In Line', 3395)\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "0.051831688596491225\n",
      "0.04890844298245614\n",
      "0.8992598684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/users/s1155124614/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# def lemmatize_words(words):\n",
    "\n",
    "#     lemmatized_words = [WordNetLemmatizer().lemmatize(word, '.') for word in words]\n",
    "    \n",
    "#     return lemmatized_words\n",
    "nltk.download(\"sentiwordnet\")\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "string1 = \"ITEM 7\"\n",
    "string2 = \"ITEM 7A\"\n",
    "f1 = open(\"2021.txt\",encoding = \"latin1\")\n",
    "index = 0\n",
    "index_list = []\n",
    "end = 0\n",
    "end_list = []\n",
    "for line in f1:\n",
    "    index += 1\n",
    "    if string1 in line:\n",
    "        print(('String', string1, 'Found In Line', index))\n",
    "        index_list.append(index)\n",
    "        continue\n",
    "f2 = open(\"2021.txt\", encoding = \"latin1\")\n",
    "for l in f2:\n",
    "    end += 1\n",
    "    if string2 in l:\n",
    "        print(('String', string2, 'Found In Line', end))\n",
    "        end_list.append(end)\n",
    "        continue\n",
    "start = index_list[0]\n",
    "end1 = end_list[0]\n",
    "content = range(start, end1)\n",
    "\n",
    "f3 = open('2021.txt', encoding = \"latin1\")\n",
    "lines = []\n",
    "for pos, l in enumerate(f3):\n",
    "    if pos in content:\n",
    "        lines.append(l)\n",
    "    continue\n",
    "\n",
    "new = []\n",
    "for l in lines:\n",
    "    new_line = l.split(\"\\n\")\n",
    "    new += new_line\n",
    "\n",
    "non_empty_lines = [n for n in new if\n",
    "                   n.strip() != \"\" and n.strip() != \"------------------------------------------------------------------------\" and n.strip() != \"Table of Contents <#toc>\"]\n",
    "string_without_empty_lines = \"\"\n",
    "for line in non_empty_lines:\n",
    "    string_without_empty_lines += line + \"\\n\"\n",
    "string_without_empty_lines = string_without_empty_lines.strip(\"\\n\").strip(\"n\").strip(\"\\t\")\n",
    "#print(string_without_empty_lines)\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = nltk.word_tokenize(string_without_empty_lines)\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokens) \n",
    "#print(tagged_tokens)\n",
    "def pos_conv(pos):\n",
    "    tag_dict = {\"NN\": \"n\",\n",
    "    \"VB\": \"v\",\n",
    "    \"JJ\": \"a\",\n",
    "    \"RB\": \"r\",\n",
    "    }\n",
    "    return tag_dict.get(pos[1])\n",
    "           \n",
    "wordnet_tags = [[t[0], pos_conv(t)] for t in tagged_tokens]\n",
    "# print(wordnet_tags)\n",
    "a = []\n",
    "for i in range(len(wordnet_tags)):\n",
    "    if wordnet_tags[i][1] != None:\n",
    "        a.append(wordnet_tags[i])\n",
    "#print(a)\n",
    "neg = []\n",
    "pos = []\n",
    "sub = []\n",
    "for i in range(len(a)):\n",
    "    try:\n",
    "        sen = swn.senti_synset(a[i][0] + \".\" + a[i][1] + \".\" + \"01\")\n",
    "        neg.append(sen.neg_score())\n",
    "        pos.append(sen.pos_score())\n",
    "        sub.append(sen.obj_score())\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "print(sum(pos)/len(pos))\n",
    "print(sum(neg)/len(neg))\n",
    "print(sum(sub)/len(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134f884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.875, 0.125)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "a = swn.senti_synset(\"happy\" + \".\" + \"a\" + \".\" + \"01\")\n",
    "# print(a)\n",
    "a.neg_score(), a.pos_score(), a.obj_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
